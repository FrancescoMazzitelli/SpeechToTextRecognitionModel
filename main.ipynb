{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports e path dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\panne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lazy_loader\\__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "C:\\Users\\panne\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\lazy_loader\\__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import backendHelper as b\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import csv\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from IPython import display\n",
    "\n",
    "import importlib\n",
    "\n",
    "# Percorsi dei file contenenti informazioni sull'associazione audio-trascrizione\n",
    "audio_base_path = \"Data\\\\it\\\\clips\"\n",
    "train_doc_path = \"Data\\\\it\\\\train.tsv\"\n",
    "test_doc_path = \"Data\\\\it\\\\test.tsv\"\n",
    "validation_doc_path = \"Data\\\\it\\\\dev.tsv\"\n",
    "\n",
    "# Impostazione di un seed statico per riprodurre l'esperimento\n",
    "seed = 42\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caricamento dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(df1, df2):\n",
    "    audio_path = []\n",
    "    transcriptions = []\n",
    "    for audio in df1:\n",
    "        audio_full_path = os.path.join(audio_base_path, audio)\n",
    "        audio_path.append(audio_full_path)\n",
    "\n",
    "    for transcript in df2:\n",
    "        transcriptions.append(transcript)\n",
    "\n",
    "    return audio_path, transcriptions\n",
    "\n",
    "train_df = pd.read_csv(train_doc_path, sep='\\t')\n",
    "test_df = pd.read_csv(test_doc_path, sep='\\t')\n",
    "validation_df = pd.read_csv(validation_doc_path, sep='\\t')\n",
    "\n",
    "train_audio_path, train_audio_transcript = load_data(train_df['path'], train_df['sentence'])\n",
    "test_audio_path, test_audio_transcript = load_data(test_df['path'], test_df['sentence'])\n",
    "validation_audio_path, validation_audio_transcript = load_data(validation_df['path'], validation_df['sentence'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione del dataset di TRAIN con tracce audio e trascrizioni + zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(b)\n",
    "sample_rate = 48000\n",
    "max_length = 200000\n",
    "csv_file_path = \"PROCESSED\\\\train.csv\"\n",
    "mfcc = []\n",
    "transcription = []\n",
    "\n",
    "for audio, t in zip(train_audio_path, train_audio_transcript):\n",
    "    mfcc_features = b.extract_mfcc(audio)\n",
    "    padded_mfcc = pad_sequences(mfcc_features, maxlen=max_length, padding='post', dtype='float32', value=0.0)\n",
    "    mfcc.append(padded_mfcc.tolist())\n",
    "    transcription.append(t)\n",
    "\n",
    "data = {\n",
    "    'mfcc': mfcc,\n",
    "    'transcription': transcription\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file_path, index=False, header=['mfcc', 'transcription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione del dataset di TEST con tracce audio e trascrizioni + + zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nplt.imshow(mfcc_features, cmap=\\'viridis\\', aspect=\\'auto\\')\\nplt.xlabel(\\'Tempo\\')\\nplt.ylabel(\\'Frequenza\\')\\nplt.title(\\'Spettrogramma\\')\\nplt.grid(False)\\n\\n# Salva l\\'immagine\\nplt.savefig(\"SPECTROGRAMS\\\\test\\\\\" + filename + \\'.png\\') \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(b)\n",
    "sample_rate = 48000\n",
    "max_length = 200000\n",
    "csv_file_path = \"PROCESSED\\\\test.csv\"\n",
    "mfcc = []\n",
    "transcription = []\n",
    "\n",
    "\n",
    "for audio, t in zip(test_audio_path, test_audio_transcript):\n",
    "    #waveform = b.get_waveform(audio)\n",
    "    #spectrogram = b.audio_to_spectrogram(waveform, sample_rate)\n",
    "    #spectrogram_str = np.array2string(spectrogram, separator=', ', threshold=np.inf)\n",
    "    #writer.writerow({'waveform': spectrogram_str, 'label': t})\n",
    "    mfcc_features = b.extract_mfcc(audio)\n",
    "    padded_mfcc = pad_sequences(mfcc_features, maxlen=max_length, padding='post', dtype='float32', value=0.0)\n",
    "    mfcc.append(padded_mfcc.tolist())\n",
    "    transcription.append(t)\n",
    "\n",
    "data = {\n",
    "    'mfcc': mfcc,\n",
    "    'transcription': transcription\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file_path, index=False, header=['mfcc', 'transcription'])\n",
    "'''\n",
    "plt.imshow(mfcc_features, cmap='viridis', aspect='auto')\n",
    "plt.xlabel('Tempo')\n",
    "plt.ylabel('Frequenza')\n",
    "plt.title('Spettrogramma')\n",
    "plt.grid(False)\n",
    "\n",
    "# Salva l'immagine\n",
    "plt.savefig(\"SPECTROGRAMS\\\\test\\\\\" + filename + '.png') \n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione del dataset di VALIDATION con tracce audio e trascrizioni + zero padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(b)\n",
    "sample_rate = 48000\n",
    "max_length = 200000\n",
    "csv_file_path = \"PROCESSED\\\\validation.csv\"\n",
    "mfcc = []\n",
    "transcription = []\n",
    "\n",
    "for audio, t in zip(validation_audio_path, validation_audio_transcript):\n",
    "    mfcc_features = b.extract_mfcc(audio)\n",
    "    padded_mfcc = pad_sequences(mfcc_features, maxlen=max_length, padding='post', dtype='float32', value=0.0)\n",
    "    mfcc.append(padded_mfcc.tolist())\n",
    "    transcription.append(t)\n",
    "\n",
    "padded_mfcc = pad_sequences(mfcc, maxlen=max_length, padding='post', dtype='float32', value=0.0)\n",
    "data = {\n",
    "    'mfcc': mfcc,\n",
    "    'transcription': transcription\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(csv_file_path, index=False, header=['mfcc', 'transcription'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modellazione e Addestramento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Caricamento dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_path = \"PROCESSED/train.csv\"\n",
    "processed_test_path = \"PROCESSED/test.csv\"\n",
    "processed_validation_path = \"PROCESSED/validation.csv\"\n",
    "\n",
    "processed_train_df = pd.read_csv(processed_train_path)\n",
    "processed_test_df = pd.read_csv(processed_test_path)\n",
    "processed_validation_df = pd.read_csv(processed_validation_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Padding per la definizione di input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148210\n"
     ]
    }
   ],
   "source": [
    "#mfcc_matrix = processed_train_df[0].values.astype(float)\n",
    "#print(processed_test_df['mfcc'])\n",
    "#print(processed_validation_df['transcription'])\n",
    "print(len(processed_train_df['mfcc'][14]))\n",
    "max_length = 200000\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creazione modello CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTMmodel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lstm \u001b[39m=\u001b[39m LSTMmodel(max_seq_length, num_classes)\n\u001b[0;32m      2\u001b[0m lstm\u001b[39m.\u001b[39msummary()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSTMmodel' is not defined"
     ]
    }
   ],
   "source": [
    "num_labels = len(processed_train_df['transcription'])\n",
    "input_shape = 200000\n",
    "model = models.Sequential([\n",
    "    layers.Input(shape=input_shape),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(num_labels),\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compilazione del modello"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addestramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
